{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online store \"Wikishop\" launches a new service.\n",
    "<br>Now users can edit and complete product descriptions, just like in wiki communities.\n",
    "<br>That is, clients propose their edits and comment on the changes of others.\n",
    "<br>The store needs a tool that will look for toxic comments and send them for moderation.\n",
    "It is necessary to train the model to classify comments into positive and negative.\n",
    "<br>There is a data set with markup on the toxicity of edits.\n",
    "<br>The model must have a quality metric F1 of at least 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in the `/datasets/toxic_comments.csv` file.\n",
    "<br>The `text` column contains the text of the comment, and `toxic` is the target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download and prepare data.\n",
    "2. Train different models.\n",
    "3. Draw conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm import notebook\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertConfig, TFBertForSequenceClassification\n",
    "\n",
    "pd.set_option('display.max_row', 100)\n",
    "pd.set_option('display.max_columns',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and model training without using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_clear(text):\n",
    "    stem_text = eng_stemmer.stem(text)   \n",
    "    stem_text = re.sub(r'[^a-zA-Z]', ' ', stem_text)   \n",
    "    return \" \".join(stem_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stem_text'] = df['text'].progress_apply(stem_clear)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df['stem_text'].values.astype('U')\n",
    "target = df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T14:17:33.386019Z",
     "start_time": "2022-11-23T14:17:33.383013Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.25, random_state=12345)\n",
    "\n",
    "print(features.shape)\n",
    "print(target.shape)\n",
    "print()\n",
    "\n",
    "print(features_train.shape)\n",
    "print(target_train.shape)\n",
    "print()\n",
    "\n",
    "print(features_test.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf_train = count_tf_idf.fit_transform(features_train)\n",
    "tf_idf_test = count_tf_idf.transform(features_test)\n",
    "count_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "model.fit(tf_idf_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(tf_idf_train)\n",
    "print('F1 Logistic Regression on train set:', f1_score(target_train, predictions))\n",
    "\n",
    "predictions = model.predict(tf_idf_test)\n",
    "print('F1 Logistic Regression on test set:', f1_score(target_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy='stratified', random_state=12345)\n",
    "dummy_clf.fit(tf_idf_train, target_train)\n",
    "predictions = dummy_clf.predict(tf_idf_test)\n",
    "print('F1 dummy on test set:', f1_score(target_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'f1',\n",
    "    'learning_rate': 0.005,\n",
    "    'verbose': 0,\n",
    "    \"max_depth\": 20,\n",
    "    \"num_iterations\": 5000,\n",
    "    \"n_estimators\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbm = lgb.LGBMClassifier(**hyper_params)\n",
    "gbm.fit(tf_idf_train, target_train, verbose=0)\n",
    "gbm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gbm.predict(tf_idf_train)\n",
    "print('F1 LightGBM on the train set:', f1_score(target_train, predictions))\n",
    "\n",
    "predictions = gbm.predict(tf_idf_test)\n",
    "print('F1 LightGBM on the test set:', f1_score(target_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did not use a decision tree and a random forest, since they require vectors of the same length, which in this case is not very good. I tried to train the model, but even the decision tree takes a very long time to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the results on the test dataset, LightGBM takes first place, and logistic regression is in second place.\n",
    "<br>But logistic regression wins in execution time.\n",
    "<br>If the class balancing parameter is not used in the logistic regression, then F1 is less than 0.75.\n",
    "<br>I think LightGBM can show the result better, but then you need to change the hyperparameters, which will lead to an increase in execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and model training using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "tokenized = df['text'].progress_apply(\n",
    "    lambda x: tokenizer.encode(x, add_special_tokens=True, max_length = 512, truncation = True))\n",
    "\n",
    "max_len = 0\n",
    "for i in notebook.tqdm(tokenized.values):\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('bert-base-cased') \n",
    "model = transformers.BertModel(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "embeddings = []\n",
    "\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "    \n",
    "    batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)])\n",
    "    attention_mask_batch = torch.LongTensor(\n",
    "    attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "    embeddings.append(batch_embeddings[0][:,0,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate(embeddings)\n",
    "target = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_target, test_target = train_test_split(features, target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(tf_idf_train)\n",
    "print('F1 BERT on the train set:', f1_score(target_train, predictions))\n",
    "\n",
    "predictions = model.predict(tf_idf_test)\n",
    "print('F1 BERT on the test set:', f1_score(target_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
