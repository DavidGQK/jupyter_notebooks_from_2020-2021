{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers began to leave Beta-Bank. Every month. A little, but noticeable. Bank marketers considered: to save\n",
    "current customers are cheaper than acquiring new ones. <br />\n",
    "<br />\n",
    "It is necessary to predict whether the client will leave the bank in the near future or not. You are provided with historical behavioral data\n",
    "customers and termination of agreements with the bank. <br />\n",
    "<br />\n",
    "It is necessary to build a model with an extremely large value of the F1-measure. You need to bring the metric to 0.59 and check it (F1-measure)\n",
    "on the test set. Also additionally calculate the AUC-ROC and compare its value with the F1-measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signs: <br />\n",
    "- RowNumber - row index in the data\n",
    "- CustomerId — unique customer identifier\n",
    "- Surname - surname\n",
    "- CreditScore - credit rating\n",
    "- Geography - country of residence\n",
    "- Gender - gender\n",
    "- Age - age\n",
    "- Tenure - the amount of real estate the client has\n",
    "- Balance - account balance\n",
    "- NumOfProducts - the number of bank products used by the client\n",
    "- HasCrCard - the presence of a credit card\n",
    "- IsActiveMember - client activity\n",
    "- EstimatedSalary - estimated salary\n",
    "\n",
    "Target sign: <br />\n",
    "- Exited - the fact that the client left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data files, study general information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('Churn.csv', sep=',')\n",
    "df = pd.read_csv('/datasets/Churn.csv', sep=',')\n",
    "display(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of unnecessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are gaps in the Tenure feature, 9 in total. Let's get rid of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df.query('Tenure == \"NaN\"').index\n",
    "df = df.drop(index).reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform categorical features into quantitative ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohe = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ohe.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the samples into sets with features and a target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_ohe.drop(['Exited'], axis=1)\n",
    "target = df_ohe['Exited']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's divide the data into three samples: train, validation test in the ratio `3 : 1 : 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_train, features_valid = train_test_split(features, test_size=0.20, random_state=12345)\n",
    "features_train, features_test = train_test_split(features_train, test_size=0.25, random_state=12345)\n",
    "\n",
    "target_train, target_valid = train_test_split(target, test_size=0.20, random_state=12345)\n",
    "target_train, target_test = train_test_split(target_train, test_size=0.25, random_state=12345)\n",
    "\n",
    "print(features.shape)\n",
    "print(features_train.shape)\n",
    "print(features_valid.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train.loc[:, numeric])\n",
    "features_train.loc[:, numeric] = scaler.transform(features_train.loc[:, numeric])\n",
    "features_valid.loc[:, numeric] = scaler.transform(features_valid.loc[:, numeric])\n",
    "features_test.loc[:, numeric] = scaler.transform(features_test.loc[:, numeric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring models without class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for depth in range(1, 12):\n",
    "    \n",
    "    model_dec_tree = DecisionTreeClassifier(random_state=12345, max_depth=depth) \n",
    "    model_dec_tree.fit(features_train, target_train)\n",
    "    predicted_valid = model_dec_tree.predict(features_valid)\n",
    "    result = accuracy_score(target_valid, predicted_valid)\n",
    "    \n",
    "    print('max_depth =', depth)\n",
    "    print(\"F1:\", f1_score(target_valid, predicted_valid))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the model with better parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dec_tree = DecisionTreeClassifier(random_state=12345, max_depth=9) \n",
    "model_dec_tree.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for est in range(10, 102, 5):\n",
    "    \n",
    "    model_ran_forest = RandomForestClassifier(random_state=12345, n_estimators=est, max_depth=9)\n",
    "    model_ran_forest.fit(features_train, target_train)\n",
    "    predicted_valid = model_ran_forest.predict(features_valid)\n",
    "    \n",
    "    print('n_estimators =', est)\n",
    "    print(\"F1:\", f1_score(target_valid, predicted_valid))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the model with better parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ran_forest = RandomForestClassifier(random_state=12345, n_estimators=45, max_depth=9)\n",
    "model_ran_forest.fit(features_train, target_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log_reg = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model_log_reg.fit(features_train, target_train)\n",
    "predicted_valid = model_log_reg.predict(features_valid)\n",
    "\n",
    "print(\"F1:\", f1_score(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking models on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = model_dec_tree.predict(features_test)\n",
    "\n",
    "print(\"Decision tree model on the test sample:\")\n",
    "print(\"F1:\", f1_score(target_test, predicted_test))\n",
    "\n",
    "probabilities_test = model_dec_tree.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "fpr_tree, tpr_tree, thresholds_tree = roc_curve(target_test, probabilities_one_test) \n",
    "print('auc_roc:', roc_auc_score(target_test, probabilities_one_test))\n",
    "print()\n",
    "#//\n",
    "#//\n",
    "predicted_test = model_ran_forest.predict(features_test)\n",
    "\n",
    "print(\"Random forest model on a test sample:\")\n",
    "print(\"F1:\", f1_score(target_test, predicted_test))\n",
    "\n",
    "probabilities_test = model_ran_forest.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "fpr_for, tpr_for, thresholds_for = roc_curve(target_test, probabilities_one_test) \n",
    "print('auc_roc:', roc_auc_score(target_test, probabilities_one_test))\n",
    "print()\n",
    "#//\n",
    "#//\n",
    "predicted_test = model_log_reg.predict(features_test)\n",
    "\n",
    "print(\"Logistic regression model on a test sample:\")\n",
    "print(\"F1:\", f1_score(target_test, predicted_test))\n",
    "\n",
    "probabilities_test = model_log_reg.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "fpr_reg, tpr_reg, thresholds_reg = roc_curve(target_test, probabilities_one_test) \n",
    "print('auc_roc:', roc_auc_score(target_test, probabilities_one_test))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the ROC curves of the models and the ROC curve of the random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.plot(fpr_tree, tpr_tree, label='Tree')\n",
    "plt.plot(fpr_for, tpr_for, label='Forest')\n",
    "plt.plot(fpr_reg, tpr_reg, label='Log_regression')\n",
    "\n",
    "# ROC-curve of a random model\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "# ROC-curve of a random model\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "\n",
    "# label the axes \"False Positive Rate\" and \"True Positive Rate\"\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# add title \"ROC curve\" with plt.title() function\n",
    "plt.title('ROC-кривая')\n",
    "\n",
    "# add a legend\n",
    "legend = plt.legend(loc='lower right', shadow=False, fontsize='x-large')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: <br />\n",
    "<br />\n",
    "The decision tree has the best F1 measure, followed by the random forest. <br />\n",
    "Random Forest has the largest area under the curve, but Decisive Forest comes in second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of models taking into account the imbalance of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data in the same way as before, only take into account the imbalance of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the samples into sets with features and a target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_ohe.drop(['Exited'], axis=1)\n",
    "target = df_ohe['Exited']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's divide the data into three samples: train, validation test in the ratio `3 : 1 : 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_valid = train_test_split(features, test_size=0.20, random_state=12345)\n",
    "features_train, features_test = train_test_split(features_train, test_size=0.25, random_state=12345)\n",
    "\n",
    "target_train, target_valid = train_test_split(target, test_size=0.20, random_state=12345)\n",
    "target_train, target_test = train_test_split(target_train, test_size=0.25, random_state=12345)\n",
    "\n",
    "print(features.shape)\n",
    "print(features_train.shape)\n",
    "print(features_valid.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the imbalance of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_frequency = df_ohe['Exited'].value_counts(normalize=True)\n",
    "print(class_frequency)\n",
    "class_frequency.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We balance the samples by increasing the rare class. It was also possible to balance them by setting the `class_weight='balanced'` parameter when setting up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_zeros = features_train[target_train == 0]\n",
    "features_ones = features_train[target_train == 1]\n",
    "target_zeros = target_train[target_train == 0]\n",
    "target_ones = target_train[target_train == 1]\n",
    "\n",
    "repeat = 4\n",
    "features_train = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "target_train = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "\n",
    "shuffle(features_train, random_state=12345)\n",
    "shuffle(target_train, random_state=12345)\n",
    "\n",
    "print(features_train[target_train == 0].shape)\n",
    "print(features_train[target_train == 1].shape)\n",
    "\n",
    "class_frequency = target_train.value_counts(normalize=True)\n",
    "print(class_frequency)\n",
    "class_frequency.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train.loc[:, numeric])\n",
    "features_train.loc[:, numeric] = scaler.transform(features_train.loc[:, numeric])\n",
    "features_valid.loc[:, numeric] = scaler.transform(features_valid.loc[:, numeric])\n",
    "features_test.loc[:, numeric] = scaler.transform(features_test.loc[:, numeric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for depth in range(1, 12):\n",
    "    \n",
    "    model_dec_tree = DecisionTreeClassifier(random_state=12345, max_depth=depth) \n",
    "    model_dec_tree.fit(features_train, target_train)\n",
    "    predicted_valid = model_dec_tree.predict(features_valid)\n",
    "    \n",
    "    print('max_depth =', depth)\n",
    "    print(\"F1:\", f1_score(target_valid, predicted_valid))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the model with better parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dec_tree = DecisionTreeClassifier(random_state=12345, max_depth=6) \n",
    "model_dec_tree.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for est in range(10, 102, 5):\n",
    "    \n",
    "    model_ran_forest = RandomForestClassifier(random_state=12345, n_estimators=est, max_depth=6)\n",
    "    model_ran_forest.fit(features_train, target_train)\n",
    "    predicted_valid = model_ran_forest.predict(features_valid)\n",
    "    \n",
    "    print('n_estimators =', est)\n",
    "    print(\"F1:\", f1_score(target_valid, predicted_valid))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the model with better parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ran_forest = RandomForestClassifier(random_state=12345, n_estimators=95, max_depth=6)\n",
    "model_ran_forest.fit(features_train, target_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log_reg = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model_log_reg.fit(features_train, target_train)\n",
    "predicted_valid = model_log_reg.predict(features_valid)\n",
    "result = model_log_reg.score(features_valid, target_valid)\n",
    "\n",
    "print(\"F1:\", f1_score(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking models on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = model_dec_tree.predict(features_test)\n",
    "\n",
    "print(\"Decision tree model on the test sample:\")\n",
    "print(\"F1:\", f1_score(target_test, predicted_test))\n",
    "\n",
    "probabilities_test = model_dec_tree.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "fpr_tree, tpr_tree, thresholds_tree = roc_curve(target_test, probabilities_one_test) \n",
    "print('auc_roc:', roc_auc_score(target_test, probabilities_one_test))\n",
    "\n",
    "print()\n",
    "#//\n",
    "#//\n",
    "predicted_test = model_ran_forest.predict(features_test)\n",
    "\n",
    "print(\"Random forest model on a test sample:\")\n",
    "print(\"F1:\", f1_score(target_test, predicted_test))\n",
    "\n",
    "probabilities_test = model_ran_forest.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "fpr_for, tpr_for, thresholds_for = roc_curve(target_test, probabilities_one_test) \n",
    "print('auc_roc:', roc_auc_score(target_test, probabilities_one_test))\n",
    "\n",
    "print()\n",
    "#//\n",
    "#//\n",
    "predicted_test = model_log_reg.predict(features_test)\n",
    "\n",
    "print(\"Logistic regression model on a test sample:\")\n",
    "print(\"F1:\", f1_score(target_test, predicted_test))\n",
    "\n",
    "probabilities_test = model_log_reg.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "fpr_reg, tpr_reg, thresholds_reg = roc_curve(target_test, probabilities_one_test) \n",
    "print('auc_roc:', roc_auc_score(target_test, probabilities_one_test))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the ROC curves of the models and the ROC curve of the random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.plot(fpr_tree, tpr_tree, label='Tree')\n",
    "plt.plot(fpr_for, tpr_for, label='Forest')\n",
    "plt.plot(fpr_reg, tpr_reg, label='Log_regression')\n",
    "\n",
    "# ROC-кривая случайной модели\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "# установимграницы осей от 0 до 1 >\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "\n",
    "# подпишем оси \"False Positive Rate\" и \"True Positive Rate\" >\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# добавим заголовок \"ROC-кривая\" функцией plt.title() >\n",
    "plt.title('ROC-curve')\n",
    "\n",
    "# добавим легенду\n",
    "legend = plt.legend(loc='lower right', shadow=False, fontsize='x-large')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: <br />\n",
    "<br />\n",
    "The random forest has the best F1 measure, as does the area under the curve. <br />\n",
    "Not much, but the decision tree lags behind. <br />\n",
    "Logistic regression shows the worst results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
